{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d9087f-bc3c-4f47-9e93-2db7ce9f40c7",
   "metadata": {},
   "source": [
    "#### Utilizzo di **TensorBoard** per la visualizzazione degli embedding addestrati su un particolare problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbbaec5-e3f8-44b4-acae-55b9fa0433c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be7a5c-3a0a-4958-b94f-25d511a54ea0",
   "metadata": {},
   "source": [
    "#### Importo un dataset (Composto da commenti) da tensorflow dataset, mescolo i dati e applico un padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20309c6e-4161-4dde-94ad-31fc2ab81051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\claud\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0baedac92247cab2bbd45d868a16ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3b131e3d8d4c3e814fd51eadae5233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\claud\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incomplete4SQZ4U\\imdb_reviews-train…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\claud\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incomplete4SQZ4U\\imdb_reviews-test.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\claud\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incomplete4SQZ4U\\imdb_reviews-unsup…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\claud\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    \"imdb_reviews/subwords8k\",\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "encoder = info.features[\"text\"].encoder\n",
    "\n",
    "train_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes=((None,), ()))\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes=((None,), ()))\n",
    "train_batch, train_labels = next(iter(train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac032720-0a81-49dc-88f3-58bacd979ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabolario: 8185 termini\n"
     ]
    }
   ],
   "source": [
    "print('Vocabolario:', encoder.vocab_size, 'termini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6035605-8993-4665-b927-f1e6deb325fd",
   "metadata": {},
   "source": [
    "#### Scrivo l'architettura e compilo il modello (l'obiettivo del modello sará di riconoscere quali sono i commenti negativi all'interno dell dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7221f733-f993-412f-8ef6-d3806bc17fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          130960    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131521 (513.75 KB)\n",
      "Trainable params: 131521 (513.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        embedding, \n",
    "\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d99a8d-982d-4862-a9e6-a1d30e29e6aa",
   "metadata": {},
   "source": [
    "#### Addestro il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb70613-e45d-4b0e-bcfc-c929190824b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 14s 5ms/step - loss: 0.5664 - accuracy: 0.6454 - val_loss: 0.3361 - val_accuracy: 0.8400\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.3353 - accuracy: 0.8645 - val_loss: 0.3458 - val_accuracy: 0.8500\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.2811 - accuracy: 0.8949 - val_loss: 0.3802 - val_accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 21s 9ms/step - loss: 0.2494 - accuracy: 0.9094 - val_loss: 0.3054 - val_accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 17s 7ms/step - loss: 0.2336 - accuracy: 0.9186 - val_loss: 0.4549 - val_accuracy: 0.8700\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.2100 - accuracy: 0.9255 - val_loss: 0.5516 - val_accuracy: 0.8300\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.1989 - accuracy: 0.9296 - val_loss: 0.4433 - val_accuracy: 0.8450\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.1817 - accuracy: 0.9351 - val_loss: 0.4655 - val_accuracy: 0.8450\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.1706 - accuracy: 0.9390 - val_loss: 0.5943 - val_accuracy: 0.8450\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.1597 - accuracy: 0.9422 - val_loss: 0.6950 - val_accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_batches, epochs=10, validation_data=test_batches, validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a9ec6-1cc9-4e40-b11a-6e0a55614603",
   "metadata": {},
   "source": [
    "* Configuro la cartella per i log\n",
    "* Salvo le parole del vocabolario che sono state utilizzate nel layer di Embedding nella variabile subwords e quelle non presenti le\n",
    "  sostituisco con 'unknow'\n",
    "* Salvo i pesi associati ai vettori del layer di embedding\n",
    "* Creo un chack-point dagli Embedding addestrati\n",
    "* Visualizzazione tramite Projector in TensorBoard degli Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2d06e7-7e82-4d2b-905e-f62073c58327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione della cartella per i log\n",
    "log_dir='logs/imdb_example/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\", encoding=\"utf-8\") as f:\n",
    "    for subwords in encoder.subwords:\n",
    "        f.write(\"{}\\n\".format(subwords))\n",
    "    for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
    "        f.write(\"unknow #{}\\n\".format(unknown))\n",
    "\n",
    "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a04aa76-a36e-4f1a-8545-6dfb5551b5db",
   "metadata": {},
   "source": [
    "eseguito lo script, in una shell avviare\n",
    "Tensorboard con il seguente comando\n",
    "nella stessa cartella in cui è salvato\n",
    "questo jupyter notebook\n",
    "```\n",
    "(venv) C:\\....\\tensorboard --logdir logs/imdb_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ae477-6a20-44bd-b478-df9ea0115d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
